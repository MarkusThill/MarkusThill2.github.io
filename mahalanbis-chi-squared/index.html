<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution | Markus Thill</title> <meta name="author" content="Markus Thill"> <meta name="description" content="The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://markusthill.github.io/mahalanbis-chi-squared/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Markus </span>Thill</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution</h1> <p class="post-meta">October 3, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/mahalanobis"> <i class="fas fa-hashtag fa-sm"></i> mahalanobis</a>   <a href="/blog/tag/chi2"> <i class="fas fa-hashtag fa-sm"></i> chi2</a>     ·   <a href="/blog/category/stats"> <i class="fas fa-tag fa-sm"></i> stats</a>   <a href="/blog/category/ml"> <i class="fas fa-tag fa-sm"></i> ml</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In practice, sometimes (multivariate) Gaussian distributions are used for anomaly detection tasks (assuming that the considered data is approx. normally distributed): the parameters of the Gaussian can be estimated using maximum likelihood estimation (MLE) where the maximum likelihood estimate is the sample mean and sample covariance matrix. After the estimating the parameters of the distribution one has to specify a critical value which separates the normal data from the anomalous data. Typically, this critical value is taken from the probability density function (PDF) in such a way that it is smaller than the PDF value of all normal data points in the data set. Then a new data point can be classified as anomalous if the value of the PDF for this new point is below the critical value. Hence, the critical value specifies a boundary which is used to separate normal from anomalous data. In the univariate case the boundary separates the lower and upper tails of the Gaussian from its center (mean). In the 2-dimensional case the boundary is an ellipse around the center and in higher dimensions the boundary can be described by an ellipsoid. But what do we do, if want to find a boundary in a way that separates the most unlikely 2% of the data points from a sample from the remaining 99%. In the univariate case, this scenario is simple: We just have to compute the first percentile (1% quantile) and 99th percentile. All points that end up in the specified tails would then be classified as anomalous. For the multivariate case this is not that straightforward any longer, since our boundary has to be described by an ellipsoid. However, there is a way out of this problem, which has to do with a so called Mahalanobis-distance, as we will see in the following.</p> <p>\( \def\matr#1{\mathbf #1} \def\tp{\mathsf T} \)</p> <h2 id="prerequisites">Prerequisites</h2> <h3 id="multiplication-of-a-matrix-with-its-transpose">Multiplication of a Matrix with its Transpose</h3> <p>Generally, the product of a \(n \times \ell\) matrix \(\matr A\) and a \(\ell \times p\) matrix \(\matr B\) is defined as:</p> \[(\matr A \matr B)_{ij}=\sum_{k=1}^m \matr A_{ik} \matr B_{kj}\] <p>Then, the multiplication of a matrix \(\matr A\) with its transpose \(\matr A^\tp\) can be written as:</p> \[\begin{align} (\matr A \matr A^\mathsf T)_{ij} &amp;= \sum_{k=1}^\ell \matr A_{ik} \matr A^\mathsf T_{kj} \\ &amp;= \sum_{k=1}^\ell \matr A_{ik} \matr A_{jk} \\ \matr A \matr A^\mathsf T &amp;= \sum_{k=1}^\ell \vec a_{k} \vec a_{k}^\tp \label{eq:matrixProductWithTranspose} \end{align}\] <p>where \(\vec a_{k}\) is the \(k\)th column vector of matrix \(\matr A\). Another trivial relation required later is as follows:</p> \[\begin{align} x &amp;= \vec a^\mathsf T \vec b \\ y &amp;= \vec b^\mathsf T \vec a = x^\mathsf T=x \\ xy &amp;= \vec a^\mathsf T \vec b \, \vec b^\mathsf T \vec a = x^2 \\ &amp;= (\vec a^\mathsf T \vec b)^2 \label{eq:multOfTwoSkalars} \end{align}\] <h3 id="inverse-of-a-matrix-product">Inverse of a Matrix-Product</h3> <p>\(\begin{align} (\matr{A} \matr B)^{-1} = \matr B^{-1}\matr A^{-1} \label{eq:inverseProduct} \end{align}\) since \(\begin{align} ( \matr A \matr B)(\matr B^{-1} \matr A^{-1}) &amp;= (\matr A(\matr B \matr B^{-1})) \matr A^{-1} \\ &amp;= ( \matr A\mathbf{I}) \matr A^{-1} \\&amp;= \matr A \matr A^{-1} \\&amp;= \mathbf{I} \end{align}\)</p> <h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3> <p>For a matrix \(\matr A\) solve: \(\begin{equation} \matr{A} \vec{u} = \lambda \vec{u} \end{equation}\)</p> <p>A value \(\lambda\) which fulfills the equation is called an eigenvalue of \(\matr A\) and the corresponding vector \(\vec\mu\) is called eigenvector. When the eigenvectors of \(\matr A\) are arranged in a matrix \(\matr U\), we have:</p> \[\begin{align} \matr{A} \matr{U} &amp;= \matr{U} \matr{\Lambda}\\ \matr{A} &amp;= \matr{U} \matr{\Lambda} \matr{U}^{-1} \label{eq:eigendecomp} \end{align}\] <p>where \(\matr \Lambda\) is a diagonal matrix containing the eigenvalues \(\lambda_i\) of the corresponding eigenvectors. This representation, where the matrix is represented in terms of its eigenvalues and eigenvectors is also called eigenvalue decomposition. For symmetric matrices \(\matr{A}\), the eigenvectors are orthogonal (orthonormal) and the matrix \(\matr{U}\) is orthogonal as well (the product with its transpose is the identity matrix). In this case \(\matr{U}^{-1}=\matr{U}^{T}\), and equation \(\eqref{eq:eigendecomp}\) can be written as:</p> \[\begin{align} \matr{A} &amp;= \matr{U} \matr{\Lambda} \matr{U}^{T} \end{align}\] <p>In this case, also the square root of \(\matr A\) (written here as \(\matr A^{\frac{1}{2}}\)) – such that \(\matr A^{\frac{1}{2}}\matr A^{\frac{1}{2}}=A\) – can be easily found to be:</p> \[\begin{align} \matr A^{\frac{1}{2}} &amp;= \matr{U} \matr{\Lambda}^{\frac{1}{2}} \matr{U}^{T} \label{eq:sqrtSymMatrix} \end{align}\] <p>since</p> \[\begin{align} \matr A^{\frac{1}{2}} \cdot \matr A^{\frac{1}{2}} &amp;= \matr{U} \matr{\Lambda}^{\frac{1}{2}} \matr{U}^{T} \matr{U} \matr{\Lambda}^{\frac{1}{2}} \matr{U}^{T} \\ &amp;=\matr{U} \matr{\Lambda}^{\frac{1}{2}} \matr I \matr{\Lambda}^{\frac{1}{2}} \matr{U}^{T} \\ &amp;= \matr{U} \matr{\Lambda} \matr{U}^{T} \\ &amp;= \matr A \end{align}\] <p>The eigenvalue decomposition of the inverse of a matrix \(\matr A\) can be computed as follows, using the relation described in equation \(\eqref{eq:inverseProduct}\) and the associative property of the matrix product:</p> \[\begin{align} \matr{A}^{-1} &amp;= \big( \matr U \matr \Lambda \matr U^{-1} \big) \\ &amp;= \big( \matr U^{-1} \big)^{-1} \matr \Lambda^{-1} \matr U^{-1}\\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1}\\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{T}\label{eq:eigenvalueInverse} \\ \end{align}\] <p>Note that \(\Lambda^{-1}\) is again a diagonal matrix containing the inverse eigenvalues of \(\matr{A}\).</p> <h3 id="linear-affine-transform-of-a-normally-distributed-random-variable">Linear Affine Transform of a Normally Distributed Random Variable</h3> <p>Assume we apply a linear affine transform to a random variable \(X \thicksim N(\vec \mu_x, \Sigma_x)\) with a mean vector \(\vec\mu_x\) and a covariance matrix \(\Sigma_x\) in order to create a new random variable \(Y\):</p> \[Y=\matr A X + \vec b.\] <p>One can compute the new mean \(\vec\mu_y\) and covariance matrix \(\Sigma_y\) for \(Y\):</p> \[\begin{align} \vec \mu_y &amp;= E \{ Y \} \\ &amp;=E \{\matr A X + \vec b \} \\ &amp;= \matr A E \{\matr X \} + \vec b \\ &amp;= \matr A \vec \mu_x + \vec b \label{eq:AffineLinearTransformMean} \\ \end{align}\] <p>and</p> \[\begin{align} \matr \Sigma_y &amp;= E \{ (Y - \vec \mu_y) (Y - \vec \mu_y)^\tp \} \\ &amp;= E \{ \big[(\matr A X + \vec b) - (\matr A \vec \mu_x + \vec b) \big] \big[(\matr A X + \vec b) - (\matr A \vec \mu_x + \vec b) \big]^\tp \} \\ &amp;= E \{ \big[\matr A (X - \vec \mu_x) \big] \big[\matr A (X - \vec \mu_x ) \big]^\tp \} \\ &amp;= E \{ \matr A (X - \vec \mu_x) (X - \vec \mu_x )^\tp \matr A^\tp \} \\ &amp;= \matr A E \{ (X - \vec \mu_x) (X - \vec \mu_x )^\tp \} \matr A^\tp\\ &amp;= \matr A \matr \Sigma_x \matr A^\tp \label{eq:AffineLinearTransformCovariance}\\ \end{align}\] <h2 id="quantile-estimation-for-multivariate-gaussian-distributions">Quantile Estimation for multivariate Gaussian Distributions</h2> <ul> <li>Calculating quantiles for multivariate normal distributions is not that trivial as in the one-dimensional case, since we cannot simply compute the integral in the tails of the distribution</li> <li>The quantiles in the bivariate case can be seen as ellipses, in higher dimensions as ellipsoids</li> <li>The Mahalanobis distance is an interesting measure to describe all points on the surface of an ellipsoid.</li> <li>More formal: The usual quantile definition requires a random variable: The p-quantile for a random distribution is the value that fulfills . In the case of a multivariate normal distribution we can take the squared Mahalanobis distance between a point of the multivariate normal distribution and its mean as such a random variable. Then the p-quantile computation will answer the following question: Which value is required so that a random point fulfills ? In other words, when we pick a random point from the distribution, it will have with probability p a squared Mahalanobis distance equal or smaller than . The set of points with forms an ellipsoid.</li> <li>In a naive solution one can use a Monte Carlo approach to sample the multivariate normal distribution and compute the quantile based on the Mahalanobis distances of the elements of the sample</li> <li>However, this Monte Carlo approach is rather computationally inefficient, especially if quantiles have to be computed very often</li> <li>One can show that the squared Mahalanobis distance of a Gaussian distribution is actually Chi-Square distributed.</li> </ul> <h3 id="empirical-results-suggesting-that-the-mahalanobis-distance-is-chi-square-distributed">Empirical Results suggesting that the Mahalanobis Distance is Chi-Square distributed</h3> <p>In a Quantile-Quantile Plot one can see that quantiles of the Mahalanobis distance of a sample drawn from a Gaussian distribution is very similar to the corresponding quantiles computed on the Chi-Square distribution. The following R-script shows this:</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">Matrix</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">DIM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="n">nSample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="w">

</span><span class="n">Posdef</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">ev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
  </span><span class="n">decomp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
  </span><span class="n">Q</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.Q</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">R</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">qr.R</span><span class="p">(</span><span class="n">decomp</span><span class="p">)</span><span class="w">
  </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">R</span><span class="p">)</span><span class="w">
  </span><span class="n">ph</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w">
  </span><span class="n">O</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ph</span><span class="p">)</span><span class="w">
  </span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">O</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="n">ev</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">O</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Posdef</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">
</span><span class="n">muhat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">DIM</span><span class="p">)</span><span class="w">


</span><span class="n">sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mvrnorm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">nSample</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">muhat</span><span class="p">,</span><span class="w"> </span><span class="n">Sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sigma</span><span class="p">)</span><span class="w">
</span><span class="n">C</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">.5</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">det</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="nb">pi</span><span class="o">*</span><span class="n">Sigma</span><span class="p">))</span><span class="w">
</span><span class="n">mahaDist2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mahalanobis</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="o">=</span><span class="n">muhat</span><span class="p">,</span><span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span><span class="w">

</span><span class="c1">#</span><span class="w">
</span><span class="c1"># Interestingly, the Mahalanobis distance of samples follows a Chi-Square distribution</span><span class="w">
</span><span class="c1"># with d degrees of freedom</span><span class="w">
</span><span class="c1">#</span><span class="w">
</span><span class="n">pps</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">100+1</span><span class="p">)</span><span class="w">
</span><span class="n">qq1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="n">quantile</span><span class="p">(</span><span class="n">mahaDist2</span><span class="p">,</span><span class="w"> </span><span class="n">probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">})</span><span class="w">
</span><span class="n">qq2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">sapply</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pps</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">qchisq</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="o">=</span><span class="n">ncol</span><span class="p">(</span><span class="n">Sigma</span><span class="p">))</span><span class="w">

</span><span class="n">dat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">qEmp</span><span class="o">=</span><span class="w"> </span><span class="n">qq1</span><span class="p">,</span><span class="w"> </span><span class="n">qChiSq</span><span class="o">=</span><span class="n">qq2</span><span class="p">)</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dat</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">qEmp</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">qChiSq</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Sample quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Chi-Squared Quantile"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_abline</span><span class="p">(</span><span class="n">slope</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span></code></pre></figure> <p><img src="https://markusthill.github.io/images/Q-Q-Plot.png" alt="Picture description" class="image-center"></p> <h3 id="the-squared-mahalanobis-distance-follows-a-chi-square-distribution-more-formal-derivation">The squared Mahalanobis Distance follows a Chi-Square Distribution: More formal Derivation</h3> <p>The Mahalanobis distance between two points \(\vec x\) and \(\vec y\) is defined as</p> \[d(\vec x,\vec y) = \sqrt{(\vec x -\vec y )^\tp \matr \Sigma^{-1} (\vec x - \vec y)}\] <p>Thus, the squared Mahalanobis distance of a random vector \(\matr X\) and the center \(\vec \mu\) of a multivariate Gaussian distribution is defined as: \(\begin{align} D = d(\matr X,\vec \mu)^2 = (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) \label{eq:sqMahalanobis} \end{align}\)</p> <p>where \(\matr \Sigma\) is a \(\ell \times \ell\) covariance matrix and \(\vec \mu\) is the mean vector. In order to achieve a different representation of \(D\) one can first perform an eigenvalue decomposition on \(\matr \Sigma^{-1}\) which is (with Eq. \(\eqref{eq:eigenvalueInverse}\) and assuming orthonormal eigenvectors):</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \matr U \matr \Lambda^{-1} \matr U^{-1} \\ &amp;= \matr U \matr \Lambda^{-1} \matr U^{T} \\ \end{align}\] <p>With Eq. \(\eqref{eq:matrixProductWithTranspose}\) we get:</p> \[\begin{align} \matr \Sigma^{-1} &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \label{eq:SigmaInverseAsSum} \end{align}\] <p>where \(\vec u_{k}\) is the \(k\)th eigenvector of the corresponding eigenvalue \(\lambda_k\). Plugging \eqref{eq:SigmaInverseAsSum} back into \eqref{eq:sqMahalanobis} results in:</p> \[\begin{align} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) \\ &amp;= (\matr X -\vec \mu )^\tp \Bigg( \sum_{k=1}^\ell \lambda_k^{-1} \vec u_{k} \vec u_{k}^\tp \Bigg) (\matr X - \vec \mu ) \\ &amp;= \sum_{k=1}^\ell \lambda_k^{-1} (\matr X -\vec \mu )^\tp \vec u_{k} \vec u_{k}^\tp (\matr X - \vec \mu ) \end{align}\] <p>With Eq. \eqref{eq:multOfTwoSkalars} one gets:</p> \[\begin{align} D &amp;= \sum_{k=1}^\ell \lambda_k^{-1} \Big[ \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2\\ &amp;= \sum_{k=1}^\ell \Big[ \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp (\matr X - \vec \mu ) \Big]^2\\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align}\] <p>where \(Y_k\) is a new random variable based on an affine linear transform of the random vector \(\matr X\). According to Eq. \eqref{eq:AffineLinearTransformMean} , we have \(\matr Z = (\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)\). If we set \(\vec a_{k}^\tp = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp\) then we get \(Y_k = \vec a_{k}^\tp \matr Z = \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \matr Z\). Note that \(Y_k\) is now a random Variable drawn from a univariate normal distribution \(Y_k \thicksim N(0,\sigma_k^2)\), where, according to \eqref{eq:AffineLinearTransformCovariance}:</p> \[\begin{align} \sigma_k^2 &amp;= \vec a_{k}^\tp \Sigma \vec a_{k}\\ &amp;= \lambda_k^{-\frac{1}{2}} \vec u_{k}^\tp \Sigma \lambda_k^{-\frac{1}{2}} \vec u_{k} \\ &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} \label{eq:smallSigma} \end{align}\] <p>If we insert</p> \[\begin{align} \matr \Sigma &amp;= \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp \end{align}\] <p>into Eq. \eqref{eq:smallSigma}, we get: \(\begin{align} \sigma_k^2 &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Sigma \vec u_{k} \\ &amp;= \lambda_k^{-1} \vec u_{k}^\tp \Bigg( \sum_{j=1}^\ell \lambda_j \vec u_{j} \vec u_{j}^\tp \Bigg) \vec u_{k} \\ &amp;= \sum_{j=1}^\ell \lambda_k^{-1} \vec u_{k}^\tp \lambda_j \vec u_{j} \vec u_{j}^\tp \vec u_{k} \\ &amp;= \sum_{j=1}^\ell \lambda_k^{-1} \lambda_j \vec u_{k}^\tp \vec u_{j} \vec u_{j}^\tp \vec u_{k} \\ \end{align}\)</p> <p>Since all eigenvectors \(\vec u_{i}\) are pairwise orthonormal the dotted products \(\vec u_{k}^\tp \vec u_{j}\) and \(\vec u_{j}^\tp \vec u_{k}\) will be zero for \(j \neq k\). Only for the case \(j = k\) we get: \(\begin{align} \sigma_k^2 &amp;= \lambda_k^{-1} \lambda_k \vec u_{k}^\tp \vec u_{k} \vec u_{k}^\tp \vec u_{k} \\ &amp;= \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 \\ &amp;= \lambda_k^{-1} \lambda_k ||\vec u_{k}||^2 ||\vec u_{k}||^2 \\ &amp;= 1, \end{align}\)</p> <p>since the the norm \(||\vec u_{k}||\) of a orthonormal eigenvector is equal to 1. The squared Mahalanobis distance can be expressed as:</p> \[\begin{align} D &amp;= \sum_{k=1}^\ell Y_k^2 \end{align}\] <p>where \(Y_k \thicksim N(0,1).\)</p> <p>Now the Chi-square distribution with \(\ell\) degrees of freedom is exactly defined as being the distribution of a variable which is the sum of the squares of \(\ell\) random variables being standard normally distributed. Hence, \(D\) is Chi-square distributed with \(\ell\) degrees of freedom.</p> <h3 id="derivation-based-on-the-whitening-property-of-the-mahalanobis-distance">Derivation based on the Whitening Property of the Mahalanobis Distance</h3> <p>Since the inverse \(\matr \Sigma^{-1}\) of the covariance matrix \(\matr \Sigma\) is also a symmetric matrix, its squareroot can be found – based on Eq. \eqref{eq:sqrtSymMatrix} – to be a symmetric matrix . In this case we can write the squared Mahalanobis distance as \(\begin{align} D &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-1} (\matr X - \vec \mu ) \\ &amp;= (\matr X -\vec \mu )^\tp \matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu )\\ &amp;= \Big( \matr \Sigma^{-\frac{1}{2}} (\matr X -\vec \mu ) \Big)^\tp \Big(\matr \Sigma^{-\frac{1}{2}} (\matr X - \vec \mu ) \Big) \\ &amp;= \matr Y^\tp \matr Y \\ &amp;= ||\matr Y||^2 \\ &amp;= \sum_{k=1}^\ell Y_k^2 \end{align}\)</p> <p>The multiplication \(\matr Y = \matr W \matr Z\), with \(\matr W=\matr \Sigma^{-\frac{1}{2}}\) and \(\matr Z= \matr X -\vec \mu\) is typically reffered to as a whitening transform, where in this case \(\matr W=\matr \Sigma^{-\frac{1}{2}}\) is the so called Mahalanobis (or ZCA) whitening matrix. \(\matr Y\) has zero mean, since \((\matr X - \vec \mu ) \thicksim N(\vec 0,\Sigma)\). Due to the (linear) whitening transform the new covariance matrix \(\matr \Sigma_y\) is the identity matrix \(\matr I\), as shown in the following (using the property in Eq. \eqref{eq:AffineLinearTransformCovariance}):</p> \[\begin{align} \matr \Sigma_y &amp;= \matr W \matr \Sigma \matr W^\tp \\ &amp;= \matr \Sigma^{-\frac{1}{2}} \matr \Sigma \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp \\ &amp;= \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \Big( \matr \Sigma^{-\frac{1}{2}} \Big)^\tp \\ &amp;= \matr \Sigma^{-\frac{1}{2}} \Big(\matr \Sigma^{\frac{1}{2}}\matr \Sigma^{\frac{1}{2}} \Big) \matr \Sigma^{-\frac{1}{2}} \\ &amp;= \Big(\matr \Sigma^{-\frac{1}{2}} \matr \Sigma^{\frac{1}{2}} \Big) \Big(\matr \Sigma^{\frac{1}{2}} \matr \Sigma^{-\frac{1}{2}}\Big) \\ &amp;= \matr I \end{align}\] <p>Hence, all elements \(Y_k\) in the random vector \(\matr Y\) are random variables drawn from independent normal distributions \(Y_k \thicksim N(0,1)\), which leads us to the same conclusion as before, that \(D\) is Chi-square distributed with \(\ell\) degrees of freedom.</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ThillKonenBaeck2018_1000097489" class="col-sm-8"> <div class="title">Online Adaptable Time Series Anomaly Detection with Discrete Wavelet Transforms and Multivariate Gaussian Distributions</div> <div class="author"> Markus Thill, Wolfgang Konen, and Thomas Bäck</div> <div class="periodical"> <em>Archives of Data Science, Series A (Online First)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Thill2017c" class="col-sm-8"> <div class="title">Anomaly Detection in Time Series with Discrete Wavelet Transforms and Maximum Likelihood Estimation</div> <div class="author"> Markus Thill, Wolfgang Konen, and Thomas Bäck</div> <div class="periodical"> <em>In Proceedings 27. Workshop Computational Intelligence</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Markus Thill. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 04, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>